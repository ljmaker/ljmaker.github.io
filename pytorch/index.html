<!DOCTYPE html><html lang="en-gb"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Pytorch - ljmakerportfolio</title><meta name="description" content="The main goal was to learn the basics of AI by using Pytorch, a Python machine learning library developed by&hellip;"><meta name="generator" content="Publii Open-Source CMS for Static Site"><link rel="canonical" href="https://ljmaker.github.io/pytorch/"><link rel="alternate" type="application/atom+xml" href="https://ljmaker.github.io/feed.xml" title="ljmakerportfolio - RSS"><link rel="alternate" type="application/json" href="https://ljmaker.github.io/feed.json" title="ljmakerportfolio - JSON"><meta property="og:title" content="Pytorch"><meta property="og:image" content="https://ljmaker.github.io/media/posts/8/Screenshot-from-2025-03-24-19-21-19.png"><meta property="og:image:width" content="802"><meta property="og:image:height" content="880"><meta property="og:site_name" content="ljmakerportfolio"><meta property="og:description" content="The main goal was to learn the basics of AI by using Pytorch, a Python machine learning library developed by&hellip;"><meta property="og:url" content="https://ljmaker.github.io/pytorch/"><meta property="og:type" content="article"><link rel="stylesheet" href="https://ljmaker.github.io/assets/css/style.css?v=74f838951fac6f2cfe001a386a288c7d"><script type="application/ld+json">{"@context":"http://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https://ljmaker.github.io/pytorch/"},"headline":"Pytorch","datePublished":"2025-03-27T18:33+01:00","dateModified":"2025-03-24T20:40+01:00","image":{"@type":"ImageObject","url":"https://ljmaker.github.io/media/posts/8/Screenshot-from-2025-03-24-19-21-19.png","height":880,"width":802},"description":"The main goal was to learn the basics of AI by using Pytorch, a Python machine learning library developed by&hellip;","author":{"@type":"Person","name":"Lovro Joksimović","url":"https://ljmaker.github.io/authors/lovro-joksimovic/"},"publisher":{"@type":"Organization","name":"Lovro Joksimović"}}</script><noscript><style>img[loading] {
                    opacity: 1;
                }</style></noscript></head><body class="post-template lines"><div class="container lines lines--right"><header class="header"><a href="https://ljmaker.github.io/" class="logo">ljmakerportfolio</a></header><main class="main post"><article class="content"><header class="content__inner content__header"><h1 class="content__title">Pytorch</h1><div class="content__meta"><div class="content__meta__left"><a href="https://ljmaker.github.io/authors/lovro-joksimovic/" class="invert content__author" rel="author" title="Lovro Joksimović">Lovro Joksimović</a></div><div class="content__meta__right"><time datetime="2025-03-27T18:33" class="content__date">March 27, 2025</time><div class="content__updated">Updated on <time datetime="2025-03-27T18:33" class="content__date">March 24, 2025</time></div></div></div></header><figure class="content__featured-image"><div class="content__featured-image__inner is-img-loading"><img src="https://ljmaker.github.io/media/posts/8/Screenshot-from-2025-03-24-19-21-19.png" srcset="https://ljmaker.github.io/media/posts/8/responsive/Screenshot-from-2025-03-24-19-21-19-xs.png 384w, https://ljmaker.github.io/media/posts/8/responsive/Screenshot-from-2025-03-24-19-21-19-sm.png 600w, https://ljmaker.github.io/media/posts/8/responsive/Screenshot-from-2025-03-24-19-21-19-md.png 768w, https://ljmaker.github.io/media/posts/8/responsive/Screenshot-from-2025-03-24-19-21-19-lg.png 1200w, https://ljmaker.github.io/media/posts/8/responsive/Screenshot-from-2025-03-24-19-21-19-xl.png 1600w" sizes="(min-width: 37.5em) 1600px, 80vw" loading="eager" height="880" width="802" alt=""></div></figure><div class="content__inner"><div class="content__entry"><h2 id="the-main-goal-was-to-learn-the-basics-of-ai-by-using-pytorch-a-python-machine-learning-library-developed-by-meta-ai">The main goal was to learn the basics of AI by using Pytorch, a Python machine learning library developed by Meta AI</h2><p>I made several simple projects in Pytorch by watching freeCodeCamp tutorials online. I learned the fundamentals of convolutional neural networks and some of their most important layers like linear, ReLU, MaxPool2d and Conv2d. I also learned what happens during optimisation, forward pass, backward pass (backpropagation), gradient descent and so on. Below are some of my favourite projects of this kind</p><h2 id="multiclass-classification-using-2d-points-blobs">Multiclass classification using 2d points (blobs)</h2><figure class="post__image post__image--center"><img loading="lazy" src="https://ljmaker.github.io/media/posts/8/Screenshot-from-2025-03-24-19-27-38.png" height="681" width="1200" alt="" sizes="(min-width: 37.5em) 1600px, 80vw" srcset="https://ljmaker.github.io/media/posts/8/responsive/Screenshot-from-2025-03-24-19-27-38-xs.png 384w, https://ljmaker.github.io/media/posts/8/responsive/Screenshot-from-2025-03-24-19-27-38-sm.png 600w, https://ljmaker.github.io/media/posts/8/responsive/Screenshot-from-2025-03-24-19-27-38-md.png 768w, https://ljmaker.github.io/media/posts/8/responsive/Screenshot-from-2025-03-24-19-27-38-lg.png 1200w, https://ljmaker.github.io/media/posts/8/responsive/Screenshot-from-2025-03-24-19-27-38-xl.png 1600w"></figure><pre class="line-numbers language-python"><code>import torch
from torch import nn
import matplotlib.pyplot as plt
from helper_functions import plot_decision_boundary
from sklearn.datasets import make_blobs
from sklearn.model_selection import train_test_split
from torchmetrics.classification import MulticlassF1Score

device = "cuda" if torch.cuda.is_available() else "cpu"

X_blob, y_blob = make_blobs(n_samples=1000, n_features=2, centers=4, cluster_std=1.5, random_state=42)

X_blob = torch.from_numpy(X_blob).type(torch.float)
y_blob = torch.from_numpy(y_blob).type(torch.long)

X_train, X_test, y_train, y_test = train_test_split(X_blob, y_blob, test_size=0.2, random_state=42)

class BlobModel(nn.Module):
    def __init__(self, input_features, output_features, hidden_units = 8):
        super().__init__()
        self.linear_layer_stack = nn.Sequential(
            nn.Linear(in_features=input_features, out_features=hidden_units),
            nn.ReLU(),
            nn.Linear(in_features=hidden_units, out_features=hidden_units),
            nn.ReLU(),
            nn.Linear(in_features=hidden_units, out_features=output_features)

        )
    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:
        return self.linear_layer_stack(x)

# Number of output features has to be same as the number of classes (4 blob colours)
model5 = BlobModel(input_features=2, output_features=4, hidden_units=8).to(device)
X_train, X_test = X_train.to(device), X_test.to(device)
y_train, y_test = y_train.to(device), y_test.to(device)

loss_func = nn.CrossEntropyLoss()
optimiser = torch.optim.SGD(params=model5.parameters(), lr=0.1)

torch.manual_seed(42)
torch.cuda.manual_seed(42)

epochs = 100

def accuracy(y_true, prediction):
    correct = torch.eq(y_true, prediction).sum().item()
    acc = (correct / len(prediction)) * 100
    return acc

for epoch in range (epochs):
    model5.train()
    logits = model5(X_train)
    y_pred = torch.argmax(torch.softmax(logits, dim=1),dim=1)
    loss = loss_func(logits, y_train)
    f1 = MulticlassF1Score(num_classes=4, average=None).to(device) # Way better than accuracy, combines precision and recall
    f1 = f1(y_pred, y_train)
    optimiser.zero_grad()
    loss.backward()
    optimiser.step()

    model5.eval()
    with torch.inference_mode():
        test_logits = model5(X_test)
        test_pred = torch.softmax(test_logits, dim=1).argmax(dim=1)
        test_loss = loss_func(test_logits, y_test)
        test_f1 = MulticlassF1Score(num_classes=4, average=None).to(device)
        test_f1 = test_f1(test_pred, y_test)

    if epoch % 10 == 0:
        print(f"Epoch: {epoch} | Loss: {loss:.4f} | F1 score: {f1} | Test loss: {test_loss:.4f} | Test F1 score: {test_f1}")



plt.figure(figsize=(12,6))
plt.subplot(1, 2, 1)
plt.title("Training")
plot_decision_boundary(model5, X_train, y_train)
plt.subplot(1, 2, 2)
plt.title("Testing")
plot_decision_boundary(model5, X_test, y_test)
plt.show()</code></pre><h2 id="multiclass-classification-using-images-from-mnist-and-fashionmnist-datasets">Multiclass classification using images from MNIST and FashionMNIST datasets</h2><p>&nbsp; &nbsp; &nbsp; &nbsp; Truths and prediction for images&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Confusion matrices</p><div class="gallery-wrapper"><div class="gallery" data-columns="2"><figure class="gallery__item"><a href="https://ljmaker.github.io/media/posts/8/gallery/Screenshot-from-2025-03-24-19-21-19.png" data-size="802x880"><img loading="lazy" src="https://ljmaker.github.io/media/posts/8/gallery/Screenshot-from-2025-03-24-19-21-19-thumbnail.png" height="843" width="768" alt=""></a></figure><figure class="gallery__item"><a href="https://ljmaker.github.io/media/posts/8/gallery/Screenshot-from-2025-03-24-19-21-41.png" data-size="999x818"><img loading="lazy" src="https://ljmaker.github.io/media/posts/8/gallery/Screenshot-from-2025-03-24-19-21-41-thumbnail.png" height="629" width="768" alt=""></a></figure><figure class="gallery__item"><a href="https://ljmaker.github.io/media/posts/8/gallery/Screenshot-from-2025-03-24-19-18-11.png" data-size="802x880"><img loading="lazy" src="https://ljmaker.github.io/media/posts/8/gallery/Screenshot-from-2025-03-24-19-18-11-thumbnail.png" height="843" width="768" alt=""></a></figure><figure class="gallery__item"><a href="https://ljmaker.github.io/media/posts/8/gallery/Screenshot-from-2025-03-24-19-18-36.png" data-size="1001x782"><img loading="lazy" src="https://ljmaker.github.io/media/posts/8/gallery/Screenshot-from-2025-03-24-19-18-36-thumbnail.png" height="600" width="768" alt=""></a></figure></div></div><p>Code for the MNIST dataset (code for FashionMNIST is very similar)</p><pre class="line-numbers language-python"><code>import torch
from torch import nn
from torch.utils.data import DataLoader
from torchvision import datasets
from torchvision.transforms import ToTensor
import matplotlib.pyplot as plt
from tqdm.auto import tqdm
from torchmetrics import ConfusionMatrix
from mlxtend.plotting import plot_confusion_matrix
import random

device = "cuda" if torch.cuda.is_available() else "cpu"

train_data = datasets.MNIST(root="data", train=True, download=True, transform=ToTensor(), target_transform=None)
test_data = datasets.MNIST(root="data", train=False, download=True, transform=ToTensor(), target_transform=None)

train_dataloader = DataLoader(dataset=train_data, batch_size=32, shuffle=True)
test_dataloader = DataLoader(dataset=test_data, batch_size=32, shuffle=False)

class_names = datasets.MNIST.classes

print(class_names)

class MNISTModel(nn.Module):
    def __init__(self, input_shape:int, hidden_units:int, output_shape:int):
        super().__init__()
        self.convblock1 = nn.Sequential(
            nn.Conv2d(in_channels=input_shape, out_channels=hidden_units, kernel_size=(3,3), stride=1, padding=1),
            nn.ReLU(),
            nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units, kernel_size=(3,3), stride=1, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=(2,2))
        )
        self.convblock2 = nn.Sequential(
            nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units, kernel_size=(3,3), stride=1, padding=1),
            nn.ReLU(),
            nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units, kernel_size=(3,3), stride=1, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=(2,2))
        )
        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(in_features=hidden_units*49, out_features=output_shape)
        )
    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:
        x = self.convblock1(x)
        x = self.convblock2(x)
        x = self.classifier(x)
        return x

model3 = MNISTModel(input_shape=1, hidden_units=10, output_shape=len(class_names)).to(device)

loss_func = nn.CrossEntropyLoss()
optimiser = torch.optim.SGD(params=model3.parameters(), lr=0.1)

def accuracy(y_true, y_pred):
    correct = torch.eq(y_true, y_pred).sum().item()
    acc = (correct / len(y_pred)) * 100
    return acc

epochs = 3

for epoch in tqdm(range(epochs)):
    model3.train()
    train_loss, train_acc = 0, 0
    for batch, (train_X, train_y) in enumerate(train_dataloader):
        train_X, train_y = train_X.to(device), train_y.to(device)
        train_pred = model3(train_X)
        loss = loss_func(train_pred, train_y)
        train_loss += loss
        train_acc += accuracy(y_true=train_y, y_pred=train_pred.argmax(dim=1))
        optimiser.zero_grad()
        loss.backward()
        optimiser.step()
    train_loss /= len(train_dataloader)
    train_acc /= len(train_dataloader)
    print(f"Epoch: {epoch} | Training loss: {train_loss:.5f} | Training accuracy: {train_acc:.2f}%")


def visual(test_data):
    plt.figure(figsize=(8,8))
    rows, cols = 4, 4
    for i in range(1, rows*cols+1):
        rand_image, label = test_data[random.randint(0, len(test_data))]
        rand_image = rand_image.unsqueeze(0).to(device)
        rand_pred = model3(rand_image)
        rand_pred = torch.argmax(rand_pred, dim=1)
        plt.subplot(rows, cols, i)
        if rand_pred == label:
            plt.title(f"Truth: {label} | Prediction: {rand_pred.item()}", c="g", fontsize=9)
        else:
            plt.title(f"Truth: {label} | Prediction: {rand_pred.item()}", c="r", fontsize=9)
        plt.imshow(rand_image.squeeze().cpu(), cmap="gray")
        plt.axis(False)
    plt.show()
visual(test_data)

def conf_mat(test_dataloader, test_data, device: torch.device):
    global fig, ax
    model3.eval()
    y_preds = []
    with torch.inference_mode():
        for test_X, test_y in test_dataloader:
            test_X, test_y = test_X.to(device), test_y.to(device)
            test_pred = model3(test_X)
            y_preds.append(test_pred.cpu())
        final_matrix = torch.cat(y_preds)

    confmat = ConfusionMatrix(num_classes=len(class_names), task="multiclass")
    confmat_tensor = confmat(preds=final_matrix, target=test_data.targets)

    fig, ax = plot_confusion_matrix(conf_mat=confmat_tensor.numpy(), class_names=class_names, figsize=(10, 7))
    plt.show()
conf_mat(test_dataloader, test_data, device)
</code></pre><h2 id="landmark-detection-using-mediapipe-hands-dataset">Landmark detection using MediaPipe Hands dataset</h2><p>I wanted to experiment with this dataset and make it detect the middle points between all the fingers on my hand</p><figure class="post__image post__image--center"><img loading="lazy" src="https://ljmaker.github.io/media/posts/8/Screenshot-from-2025-03-24-19-12-25.png" height="1046" width="1845" alt="" sizes="(min-width: 37.5em) 1600px, 80vw" srcset="https://ljmaker.github.io/media/posts/8/responsive/Screenshot-from-2025-03-24-19-12-25-xs.png 384w, https://ljmaker.github.io/media/posts/8/responsive/Screenshot-from-2025-03-24-19-12-25-sm.png 600w, https://ljmaker.github.io/media/posts/8/responsive/Screenshot-from-2025-03-24-19-12-25-md.png 768w, https://ljmaker.github.io/media/posts/8/responsive/Screenshot-from-2025-03-24-19-12-25-lg.png 1200w, https://ljmaker.github.io/media/posts/8/responsive/Screenshot-from-2025-03-24-19-12-25-xl.png 1600w"></figure><p></p><pre class="line-numbers language-python"><code>import cv2  # for reading image paths and drawing on images
import mediapipe as mp  # for importing the hands tracking solution
import matplotlib.pyplot as plt  # to show images

mp_hands = mp.solutions.hands
hands = mp_hands.Hands(static_image_mode=True, max_num_hands=2, min_tracking_confidence=0.7,
                       min_detection_confidence=0.7)  # finetune hyperparameters
mp_drawing = mp.solutions.drawing_utils  # utilise landmark drawing option

# read image path:
sample_img = cv2.imread("/home/nemo/PycharmProjects/ObjectDetection/data/images/2023-09-24-141225.jpg")

chosenHandPoints = [3, 7, 11, 15, 19]  # list of optimal hand points that surround the knife hit points
chosenCoordinates = []  # list for coordinates of the chosen hand points
targetCoordinates = []  # list for the coordinates of knife hit points
coordinate2 = 1

imgRGB = cv2.cvtColor(sample_img, cv2.COLOR_BGR2RGB)  # openCV uses BGR order while matplotlib uses RGB
results = hands.process(imgRGB)
if results.multi_hand_landmarks:
    for handLms in results.multi_hand_landmarks:  # to track both hands
        for points_id, lm in enumerate(handLms.landmark):
            h, w, c = sample_img.shape  # store shape of image
            x, y = int(lm.x * w), int(lm.y * h)  # turns the image ratio into actual pixel coordinates

            if points_id in chosenHandPoints:
                chosenCoordinates.append([x, y])  # store coordinates of chosen points in the list

        # used to draw connections from one point to another:
        mp_drawing.draw_landmarks(sample_img, handLms, mp_hands.HAND_CONNECTIONS)


for coordinate in range(len(chosenCoordinates)-1):
    # calculates x and y coordinates of knife hit points:
    cx = int((chosenCoordinates[coordinate][0] + chosenCoordinates[coordinate2][0]) / 2)
    cy = int((chosenCoordinates[coordinate][1] + chosenCoordinates[coordinate2][1]) / 2)
    targetCoordinates.append([cx, cy])  # stores coordinates of knife hit points into list
    coordinate2 += 1

# Calculating the locations of the hit points that are not inbetween fingers and storing them in the list:
leftDotX = int(2 * chosenCoordinates[0][0] - (chosenCoordinates[0][0] + chosenCoordinates[1][0]) / 2)
leftDotY = int(2 * chosenCoordinates[0][1] - (chosenCoordinates[0][1] + chosenCoordinates[1][1]) / 2)
targetCoordinates.insert(0, [leftDotX, leftDotY])

rightDotX = int(2 * chosenCoordinates[4][0] - (chosenCoordinates[3][0] + chosenCoordinates[4][0]) / 2)
rightDotY = int(2 * chosenCoordinates[4][1] - (chosenCoordinates[3][1] + chosenCoordinates[4][1]) / 2)
targetCoordinates.append([rightDotX, rightDotY])

# goes through the knife hit coordinates list and marks the locations on the image:
for i in range(len(targetCoordinates)):
    cv2.circle(sample_img, (targetCoordinates[i][0], targetCoordinates[i][1]), 15, (255, 0, 0), cv2.FILLED)

print(targetCoordinates)

# plotting the image using matplotlib:

plt.figure(figsize=(10, 10))
plt.axis("off")
plt.title("Resulting image")
plt.imshow(sample_img[:, :, ::-1])
plt.show()</code></pre><p></p></div><footer class="content__footer"><div class="content__tag-share"><div class="content__share"></div></div></footer></div></article></main><footer class="footer"><div class="footer__left"><div class="footer__copy">Powered by Publii</div></div><button onclick="backToTopFunction()" id="backToTop" class="footer__bttop" aria-label="Back to top" title="Back to top"><svg><use xlink:href="https://ljmaker.github.io/assets/svg/svg-map.svg#toparrow"/></svg></button></footer></div><script>window.publiiThemeMenuConfig = { mobileMenuMode: 'sidebar', animationSpeed: 300, submenuWidth: 'auto', doubleClickTime: 500, mobileMenuExpandableSubmenus: true, relatedContainerForOverlayMenuSelector: '.navbar', };</script><script defer="defer" src="https://ljmaker.github.io/assets/js/scripts.min.js?v=9c5ab7a87221183f149a42b3cceb7956"></script><script>function publiiDetectLoadedImages () {
         var images = document.querySelectorAll('img[loading]:not(.is-loaded)');
         for (var i = 0; i < images.length; i++) {
            if (images[i].complete) {
               images[i].classList.add('is-loaded');
               images[i].parentNode.classList.remove('is-img-loading');
            } else {
               images[i].addEventListener('load', function () {
                  this.classList.add('is-loaded');
                  this.parentNode.classList.remove('is-img-loading');
               }, false);
            }
         }
      }
      publiiDetectLoadedImages();</script></body></html>