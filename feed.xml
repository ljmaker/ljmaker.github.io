<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <title>ljmakerportfolio</title>
    <link href="https://ljmaker.github.io/feed.xml" rel="self" />
    <link href="https://ljmaker.github.io" />
    <updated>2025-03-31T17:09:00+02:00</updated>
    <author>
        <name>Lovro Joksimović</name>
    </author>
    <id>https://ljmaker.github.io</id>

    <entry>
        <title>KILROY</title>
        <author>
            <name>Lovro Joksimović</name>
        </author>
        <link href="https://ljmaker.github.io/kilroy-2/"/>
        <id>https://ljmaker.github.io/kilroy-2/</id>
        <media:content url="https://ljmaker.github.io/media/posts/3/KILRORYRaw.png" medium="image" />

        <updated>2025-03-31T17:09:00+02:00</updated>
            <summary>
                <![CDATA[
                        <img src="https://ljmaker.github.io/media/posts/3/KILRORYRaw.png" alt="" />
                    &nbsp; Kilroy was made from a chassis that was ordered from AliExpress, which I had to assemble myself. The materials&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                    <p><img src="https://ljmaker.github.io/media/posts/3/KILRORYRaw.png" class="type:primaryImage" alt="" /></p>
                
  <p>
    &nbsp;
  </p>

    <h2 id="the-goal-was-to-learn-how-to-control-motors-and-various-sensors-with-a-microcontroller">
      The goal was to learn how to control motors and various sensors with a microcontroller
    </h2>

  <p>
    Kilroy was made from a chassis that was ordered from AliExpress, which I had to assemble myself. The materials used were acrylic for the top layer, aluminium for the bottom layer and plastic for the tracks.&nbsp;
  </p>

  <div  class="gallery-wrapper">
    <div class="gallery" data-columns="2">
      <figure class="gallery__item">
      <a href="https://ljmaker.github.io/media/posts/3/gallery/DSC_0342.JPG" data-size="4208x2368">
        <img loading="lazy" src="https://ljmaker.github.io/media/posts/3/gallery/DSC_0342-thumbnail.JPG" height="432" width="768" alt="" >
      </a>
      
    </figure><figure class="gallery__item">
      <a href="https://ljmaker.github.io/media/posts/3/gallery/DSC_0343.JPG" data-size="4208x2368">
        <img loading="lazy" src="https://ljmaker.github.io/media/posts/3/gallery/DSC_0343-thumbnail.JPG" height="432" width="768" alt="" >
      </a>
      
    </figure>
    </div>
  </div>

    <h2 id="kilroy-had-two-modes-mode-1-was-for-remote-control-and-mode-two-for-obstacle-avoidance">
      KILROY had two modes - mode 1 was for remote control and mode two for obstacle avoidance
    </h2>

  <p>
    The final version had a light sensor for measuring light intensity and turning on LED's when it got too dark, an ultrasonic sensor for autonomous driving and obstacle avoidance, an I2C LCD display (on later model)&nbsp; that showed the current active mode and light intensity and an IR receiver module used for remote control with a TV remote. It also had a pushbutton and a switch mounted on the breadboard. The microcontroller used was Arduino UNO.&nbsp;&nbsp;
  </p>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="https://ljmaker.github.io/media/posts/3/KILROYAnnotated.png" height="739" width="1239" alt=""  sizes="(min-width: 37.5em) 1600px, 80vw" srcset="https://ljmaker.github.io/media/posts/3/responsive/KILROYAnnotated-xs.png 384w ,https://ljmaker.github.io/media/posts/3/responsive/KILROYAnnotated-sm.png 600w ,https://ljmaker.github.io/media/posts/3/responsive/KILROYAnnotated-md.png 768w ,https://ljmaker.github.io/media/posts/3/responsive/KILROYAnnotated-lg.png 1200w ,https://ljmaker.github.io/media/posts/3/responsive/KILROYAnnotated-xl.png 1600w">
      
    </figure>

  <p>
    Two motors were controlled by a now outdated L298N dual channel motor driver, which produced a lot of heat so I mounted it directly on the aluminium for cooling.
  </p>

    <h2 id="3d-model-was-made-in-fusion-360-and-the-circuit-in-fritzing">
      3d model was made in Fusion 360 and the circuit in Fritzing
    </h2>

  <p>
    This circuit was made for the first model. It didn't feature a display
  </p>

  <p>
    
  </p>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="https://ljmaker.github.io/media/posts/3/KILROYCircuit.png" height="822" width="783" alt=""  sizes="(min-width: 37.5em) 1600px, 80vw" srcset="https://ljmaker.github.io/media/posts/3/responsive/KILROYCircuit-xs.png 384w ,https://ljmaker.github.io/media/posts/3/responsive/KILROYCircuit-sm.png 600w ,https://ljmaker.github.io/media/posts/3/responsive/KILROYCircuit-md.png 768w ,https://ljmaker.github.io/media/posts/3/responsive/KILROYCircuit-lg.png 1200w ,https://ljmaker.github.io/media/posts/3/responsive/KILROYCircuit-xl.png 1600w">
      
    </figure>

  <p>
    The robot worked as intended, with the only problem being that I couldn't mount the display to the acrylic so it kept falling off. This was later fixed by supergluing it. Also, the battery pack had a very short lifespan of about 2 hours max. I remember having trouble finding the right power source for this project and this one definitely wasn't ideal.&nbsp;
  </p>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="https://ljmaker.github.io/media/posts/3/KILROYPic.JPG" height="2368" width="4208" alt=""  sizes="(min-width: 37.5em) 1600px, 80vw" srcset="https://ljmaker.github.io/media/posts/3/responsive/KILROYPic-xs.JPG 384w ,https://ljmaker.github.io/media/posts/3/responsive/KILROYPic-sm.JPG 600w ,https://ljmaker.github.io/media/posts/3/responsive/KILROYPic-md.JPG 768w ,https://ljmaker.github.io/media/posts/3/responsive/KILROYPic-lg.JPG 1200w ,https://ljmaker.github.io/media/posts/3/responsive/KILROYPic-xl.JPG 1600w">
      
    </figure>
<div><iframe loading="lazy" 
  src="https://drive.google.com/file/d/1OdtUh5tx8QpUVuGK8y5YTkqmVqAv6cIO/preview" 
  width="640" 
  height="360" 
  frameborder="0" 
  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" 
  allowfullscreen>
</iframe>
</div>

    <h2 id="the-old-code-was-very-simple-made-in-arduino-ide">
      The old code was very simple, made in Arduino IDE
    </h2>
<pre class="line-numbers  language-cpp"><code>#include &lt;IRremote.h&gt;
#include &lt;Wire.h&gt;
#include &lt;LiquidCrystal_I2C.h&gt;
#include &lt;NewPing.h&gt;

#define IR_RECEIVER_PIN 2
#define LIGHT_SENSOR_PIN 5
#define ULTRASONIC_TRIGGER_PIN 3
#define ULTRASONIC_ECHO_PIN 4
#define PUSHBUTTON_PIN 6
#define MOTOR1_PIN1 13
#define MOTOR1_PIN2 12
#define MOTOR2_PIN1 11
#define MOTOR2_PIN2 10
#define ENABLE_PIN_M1 9
#define ENABLE_PIN_M2 8
#define LCD_SWITCH_PIN 7 
#define LED1_PIN A0      
#define LED2_PIN A1      


#define IR_BUTTON_UP 0xFF18E7
#define IR_BUTTON_DOWN 0xFF4AB5
#define IR_BUTTON_LEFT 0xFF10EF
#define IR_BUTTON_RIGHT 0xFF5AA5
#define IR_BUTTON_STOP 0xFF38C7


#define MODE_REMOTE_CONTROL 0
#define MODE_OBSTACLE_AVOIDANCE 1

int currentMode = MODE_REMOTE_CONTROL;
bool lcdOn = true;
unsigned long lastDebounceTime = 0;
unsigned long debounceDelay = 50;

IRrecv irReceiver(IR_RECEIVER_PIN);
decode_results irResults;
LiquidCrystal_I2C lcd(0x27, 16, 2); 
NewPing sonar(ULTRASONIC_TRIGGER_PIN, ULTRASONIC_ECHO_PIN, 200); 

void setup() {
  
  pinMode(MOTOR1_PIN1, OUTPUT);
  pinMode(MOTOR1_PIN2, OUTPUT);
  pinMode(MOTOR2_PIN1, OUTPUT);
  pinMode(MOTOR2_PIN2, OUTPUT);
  pinMode(ENABLE_PIN_M1, OUTPUT);
  pinMode(ENABLE_PIN_M2, OUTPUT);
  pinMode(PUSHBUTTON_PIN, INPUT_PULLUP);
  pinMode(LCD_SWITCH_PIN, INPUT_PULLUP);
  pinMode(LIGHT_SENSOR_PIN, INPUT);
  pinMode(LED1_PIN, OUTPUT);
  pinMode(LED2_PIN, OUTPUT);

  
  irReceiver.enableIRIn();

 
  lcd.begin();
  lcd.backlight();
  lcd.print("Kilroy Ready!");
  delay(1000);
  lcd.clear();
}

void loop() {
  
  if (digitalRead(LCD_SWITCH_PIN)) {
    lcdOn = true;
    lcd.backlight();
  } else {
    lcdOn = false;
    lcd.noBacklight();
  }

  
  int lightValue = analogRead(LIGHT_SENSOR_PIN);
  if (lightValue &lt; 500) { // Adjust threshold as needed
    digitalWrite(LED1_PIN, HIGH);
    digitalWrite(LED2_PIN, HIGH);
  } else {
    digitalWrite(LED1_PIN, LOW);
    digitalWrite(LED2_PIN, LOW);
  }

  
  if (digitalRead(PUSHBUTTON_PIN) == LOW && (millis() - lastDebounceTime) &gt; debounceDelay) {
    lastDebounceTime = millis();
    currentMode = !currentMode; // Toggle mode
    lcd.clear();
    lcd.print("Mode: ");
    lcd.print(currentMode == MODE_REMOTE_CONTROL ? "Remote" : "Avoid");
  }

  
  if (currentMode == MODE_REMOTE_CONTROL) {
    remoteControlMode();
  } else {
    obstacleAvoidanceMode();
  }
}

void remoteControlMode() {
  if (irReceiver.decode(&irResults)) {
    switch (irResults.value) {
      case IR_BUTTON_UP:
        moveForward();
        break;
      case IR_BUTTON_DOWN:
        moveBackward();
        break;
      case IR_BUTTON_LEFT:
        turnLeft();
        break;
      case IR_BUTTON_RIGHT:
        turnRight();
        break;
      case IR_BUTTON_STOP:
        stopMotors();
        break;
    }
    irReceiver.resume();
  }
}

void obstacleAvoidanceMode() {
  int distance = sonar.ping_cm();
  if (distance &gt; 0 && distance &lt; 20) { // Obstacle within 20cm
    stopMotors();
    delay(500);
    turnRight();
    delay(500);
  } else {
    moveForward();
  }
}

void moveForward() {
  digitalWrite(MOTOR1_PIN1, HIGH);
  digitalWrite(MOTOR1_PIN2, LOW);
  digitalWrite(MOTOR2_PIN1, HIGH);
  digitalWrite(MOTOR2_PIN2, LOW);
  analogWrite(ENABLE_PIN_M1, 255);
  analogWrite(ENABLE_PIN_M2, 255);
}

void moveBackward() {
  digitalWrite(MOTOR1_PIN1, LOW);
  digitalWrite(MOTOR1_PIN2, HIGH);
  digitalWrite(MOTOR2_PIN1, LOW);
  digitalWrite(MOTOR2_PIN2, HIGH);
  analogWrite(ENABLE_PIN_M1, 255);
  analogWrite(ENABLE_PIN_M2, 255);
}

void turnLeft() {
  digitalWrite(MOTOR1_PIN1, LOW);
  digitalWrite(MOTOR1_PIN2, HIGH);
  digitalWrite(MOTOR2_PIN1, HIGH);
  digitalWrite(MOTOR2_PIN2, LOW);
  analogWrite(ENABLE_PIN_M1, 255);
  analogWrite(ENABLE_PIN_M2, 255);
}

void turnRight() {
  digitalWrite(MOTOR1_PIN1, HIGH);
  digitalWrite(MOTOR1_PIN2, LOW);
  digitalWrite(MOTOR2_PIN1, LOW);
  digitalWrite(MOTOR2_PIN2, HIGH);
  analogWrite(ENABLE_PIN_M1, 255);
  analogWrite(ENABLE_PIN_M2, 255);
}

void stopMotors() {
  digitalWrite(MOTOR1_PIN1, LOW);
  digitalWrite(MOTOR1_PIN2, LOW);
  digitalWrite(MOTOR2_PIN1, LOW);
  digitalWrite(MOTOR2_PIN2, LOW);
}</code></pre>
            ]]>
        </content>
    </entry>
    <entry>
        <title>R2D2</title>
        <author>
            <name>Lovro Joksimović</name>
        </author>
        <link href="https://ljmaker.github.io/r2d2/"/>
        <id>https://ljmaker.github.io/r2d2/</id>
        <media:content url="https://ljmaker.github.io/media/posts/6/20230716_171203-01.jpeg" medium="image" />

        <updated>2025-03-30T00:49:00+01:00</updated>
            <summary>
                <![CDATA[
                        <img src="https://ljmaker.github.io/media/posts/6/20230716_171203-01.jpeg" alt="" />
                    The main goal was to make money off of it and use up some of my components during the COVID-19&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                    <p><img src="https://ljmaker.github.io/media/posts/6/20230716_171203-01.jpeg" class="type:primaryImage" alt="" /></p>
                
    <h2 id="the-main-goal-was-to-make-money-off-of-it-and-use-up-some-of-my-components-during-the-covid-19-pandemic">
      The main goal was to make money off of it and use up some of my components during the COVID-19 pandemic
    </h2>

  <p>
    This was a simple, but in my opinion very cool project. I bought my current 3D printer Ender 3 Pro in 2019 and wanted to make some fun project with it. I decided on making a toy R2D2 with some basic components inside. I 3D printed them in bulk from an open source file from Thingiverse, painted them and put simple electronics in them. I later sold six of them on Etsy.&nbsp;
  </p>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="https://ljmaker.github.io/media/posts/6/20230716_173214.jpg" height="3000" width="4000" alt=""  sizes="(min-width: 37.5em) 1600px, 80vw" srcset="https://ljmaker.github.io/media/posts/6/responsive/20230716_173214-xs.jpg 384w ,https://ljmaker.github.io/media/posts/6/responsive/20230716_173214-sm.jpg 600w ,https://ljmaker.github.io/media/posts/6/responsive/20230716_173214-md.jpg 768w ,https://ljmaker.github.io/media/posts/6/responsive/20230716_173214-lg.jpg 1200w ,https://ljmaker.github.io/media/posts/6/responsive/20230716_173214-xl.jpg 1600w">
      
    </figure>

  <p>
    I made just a few adjustments to the 3d model, like making a hole for the blinking RGB led, a slot for an ON/OFF switch and hole for the USB port. I also 3d printed working wheels on the bottom so it could act as a real toy when turned off
  </p>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="https://ljmaker.github.io/media/posts/6/R2D2Pic2.jpg" height="3000" width="4000" alt=""  sizes="(min-width: 37.5em) 1600px, 80vw" srcset="https://ljmaker.github.io/media/posts/6/responsive/R2D2Pic2-xs.jpg 384w ,https://ljmaker.github.io/media/posts/6/responsive/R2D2Pic2-sm.jpg 600w ,https://ljmaker.github.io/media/posts/6/responsive/R2D2Pic2-md.jpg 768w ,https://ljmaker.github.io/media/posts/6/responsive/R2D2Pic2-lg.jpg 1200w ,https://ljmaker.github.io/media/posts/6/responsive/R2D2Pic2-xl.jpg 1600w">
      
    </figure>

  <p>
    The circuit consisted of an ATtiny85, a piezo buzzer which made typical R2D2 sounds and an RGB LED which blinked like an R2D2 does. The sound is from R2D2 library I didn't program it from scratch myself.
  </p>
<div><iframe loading="lazy" 
  src="https://drive.google.com/file/d/1l6KqiyxjAX_TtXpmqtpj7u3dzlGwcHvY/preview" 
  width="640" 
  height="360" 
  frameborder="0" 
  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" 
  allowfullscreen>
</iframe></div>

  <p>
    I made around a dozen of these during the beginning of the pandemic
  </p>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="https://ljmaker.github.io/media/posts/6/R2D2Circuit.jpg" height="3072" width="4080" alt=""  sizes="(min-width: 37.5em) 1600px, 80vw" srcset="https://ljmaker.github.io/media/posts/6/responsive/R2D2Circuit-xs.jpg 384w ,https://ljmaker.github.io/media/posts/6/responsive/R2D2Circuit-sm.jpg 600w ,https://ljmaker.github.io/media/posts/6/responsive/R2D2Circuit-md.jpg 768w ,https://ljmaker.github.io/media/posts/6/responsive/R2D2Circuit-lg.jpg 1200w ,https://ljmaker.github.io/media/posts/6/responsive/R2D2Circuit-xl.jpg 1600w">
      
    </figure>

  <p>
    The toy itself was not so practical to play with because it had to be plugged in with a usb cable to make sounds and the arms were somewhat fragile. I am still happy how it turned out
  </p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>Shower temp sensor</title>
        <author>
            <name>Lovro Joksimović</name>
        </author>
        <link href="https://ljmaker.github.io/shower-temp-sensor/"/>
        <id>https://ljmaker.github.io/shower-temp-sensor/</id>
        <media:content url="https://ljmaker.github.io/media/posts/4/ShowerReal2.jpg" medium="image" />

        <updated>2025-03-29T19:05:00+01:00</updated>
            <summary>
                <![CDATA[
                        <img src="https://ljmaker.github.io/media/posts/4/ShowerReal2.jpg" alt="" />
                    The goal was to learn how to design pcb's This was made as a joint project between me and my&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                    <p><img src="https://ljmaker.github.io/media/posts/4/ShowerReal2.jpg" class="type:primaryImage" alt="" /></p>
                
    <h2 id="the-goal-was-to-learn-how-to-design-pcbs">
      The goal was to learn how to design pcb's
    </h2>

  <p>
    This was made as a joint project between me and my dad. He prepared the metal casing and made the outdoor shower, and I made all the electronics.&nbsp;
  </p>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="https://ljmaker.github.io/media/posts/4/ShowerReal1.jpg" height="6144" width="8160" alt=""  sizes="(min-width: 37.5em) 1600px, 80vw" srcset="https://ljmaker.github.io/media/posts/4/responsive/ShowerReal1-xs.jpg 384w ,https://ljmaker.github.io/media/posts/4/responsive/ShowerReal1-sm.jpg 600w ,https://ljmaker.github.io/media/posts/4/responsive/ShowerReal1-md.jpg 768w ,https://ljmaker.github.io/media/posts/4/responsive/ShowerReal1-lg.jpg 1200w ,https://ljmaker.github.io/media/posts/4/responsive/ShowerReal1-xl.jpg 1600w">
      
    </figure>

  <p>
    The main components of the project included an ATtiny85 microcontroller which controlled two shift registers. These shift registers, in turn, drove two 7-segment displays to show the output. A waterproof temperature sensor was connected to the ATtiny85, and the temperature readings were displayed on the screen in degrees Celsius, rounded to the nearest whole number.
<br>
<br>For power, a 9-volt battery was used. Since the ATtiny85 has a maximum voltage rating of 5.5 volts, a voltage regulator was added to step down the voltage. The circuit also included ceramic and electrolytic capacitors to ensure stable operation. Additionally, the board featured reverse polarity protection, implemented using a simple diode to safeguard the components.
  </p>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="https://ljmaker.github.io/media/posts/4/ShowerADATopAnnot.png" height="579" width="532" alt=""  sizes="(min-width: 37.5em) 1600px, 80vw" srcset="https://ljmaker.github.io/media/posts/4/responsive/ShowerADATopAnnot-xs.png 384w ,https://ljmaker.github.io/media/posts/4/responsive/ShowerADATopAnnot-sm.png 600w ,https://ljmaker.github.io/media/posts/4/responsive/ShowerADATopAnnot-md.png 768w ,https://ljmaker.github.io/media/posts/4/responsive/ShowerADATopAnnot-lg.png 1200w ,https://ljmaker.github.io/media/posts/4/responsive/ShowerADATopAnnot-xl.png 1600w">
      <figcaption>front side of empty pcb</figcaption>
    </figure>

    <h2 id="the-final-pcb-was-designed-after-i-tested-the-functionality-on-two-different-boards">
      The final pcb was designed after I tested the functionality on two different boards
    </h2>

  <div  class="gallery-wrapper gallery-wrapper--wide">
    <div class="gallery" data-columns="3">
      <figure class="gallery__item">
      <a href="https://ljmaker.github.io/media/posts/4/gallery/TempTry0-2.png" data-size="1280x963">
        <img loading="lazy" src="https://ljmaker.github.io/media/posts/4/gallery/TempTry0-2-thumbnail.png" height="578" width="768" alt="" >
      </a>
      <figcaption>First board didn't feature an onboard microcontoller and had a button instead of a switch</figcaption>
    </figure><figure class="gallery__item">
      <a href="https://ljmaker.github.io/media/posts/4/gallery/TempTry1-2.png" data-size="1280x963">
        <img loading="lazy" src="https://ljmaker.github.io/media/posts/4/gallery/TempTry1-2-thumbnail.png" height="578" width="768" alt="" >
      </a>
      <figcaption>Second board had an arduino pro mini as an onboard microcontroller. I didn't like this design because a lot of pins weren't utilised</figcaption>
    </figure><figure class="gallery__item">
      <a href="https://ljmaker.github.io/media/posts/4/gallery/TempFinal-2.png" data-size="1280x963">
        <img loading="lazy" src="https://ljmaker.github.io/media/posts/4/gallery/TempFinal-2-thumbnail.png" height="578" width="768" alt="" >
      </a>
      <figcaption>Final design inside its case, with a waterproof switch and temperature sensor cable visible</figcaption>
    </figure>
    </div>
  </div>

    <h2 id="the-circuit-for-this-project-was-made-in-easyeda-which-was-the-first-proper-electronic-design-automation-tool-i-used">
      The circuit for this project was made in EasyEDA, which was the first proper electronic design automation tool I used
    </h2>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="https://ljmaker.github.io/media/posts/4/ShowerSchematic.png" height="717" width="1086" alt=""  sizes="(min-width: 37.5em) 1600px, 80vw" srcset="https://ljmaker.github.io/media/posts/4/responsive/ShowerSchematic-xs.png 384w ,https://ljmaker.github.io/media/posts/4/responsive/ShowerSchematic-sm.png 600w ,https://ljmaker.github.io/media/posts/4/responsive/ShowerSchematic-md.png 768w ,https://ljmaker.github.io/media/posts/4/responsive/ShowerSchematic-lg.png 1200w ,https://ljmaker.github.io/media/posts/4/responsive/ShowerSchematic-xl.png 1600w">
      
    </figure>

  <p>
    ATtiny85 was programmed using Arduino pro mini as an ISP (In-System-Programmer). The hardest part of this project in terms of software was figuring out the binary representation of each number shown on the 7-segment display, and then encoding each one into decimal (line 13 below). The rest of the code wasn't complicated.
  </p>
<pre class="line-numbers  language-cpp"><code>#include &lt;OneWire.h&gt;
#include &lt;DallasTemperature.h&gt;

#define ONE_WIRE_BUS 4

OneWire oneWire(ONE_WIRE_BUS);  
DallasTemperature sensors(&oneWire);

int DS_pin = 2;
int STCP_pin = 1; //clock
int SHCP_pin = 0 ; // latch

int dec_digits  [10] {253, 21, 219, 235, 103, 175, 191, 225, 255, 239};

float temp = 0;
float decpart = 0;
int second_digit = 0;
int first_digit = 0;

void setup() {

  pinMode(DS_pin,  OUTPUT);
  pinMode(STCP_pin, OUTPUT);
  pinMode(SHCP_pin, OUTPUT);
}


void  loop() {

  sensors.requestTemperatures();
  temp = sensors.getTempCByIndex(0);
 
  second_digit = int(temp) % 10;
  decpart = temp - int(temp);
  first_digit = int(temp / 10);
  if (decpart &gt;= 0.5){
    second_digit++;
  }
 
  digitalWrite(STCP_pin, LOW);
  shiftOut(DS_pin,  SHCP_pin, LSBFIRST, dec_digits[second_digit]);
  shiftOut(DS_pin,  SHCP_pin, LSBFIRST, dec_digits[first_digit]);
  digitalWrite(STCP_pin, HIGH);
  delay(300);


}</code></pre>
            ]]>
        </content>
    </entry>
    <entry>
        <title>ASTRO</title>
        <author>
            <name>Lovro Joksimović</name>
        </author>
        <link href="https://ljmaker.github.io/astro/"/>
        <id>https://ljmaker.github.io/astro/</id>
        <media:content url="https://ljmaker.github.io/media/posts/7/ASTROPic1-2.jpg" medium="image" />

        <updated>2025-03-28T01:07:00+01:00</updated>
            <summary>
                <![CDATA[
                        <img src="https://ljmaker.github.io/media/posts/7/ASTROPic1-2.jpg" alt="" />
                    During my junior year I was a student assistant in CRTA, a robotics lab in my faculty. I helped build&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                    <p><img src="https://ljmaker.github.io/media/posts/7/ASTROPic1-2.jpg" class="type:primaryImage" alt="" /></p>
                
  <p>
    During my junior year I was a student assistant in CRTA, a robotics lab in my faculty. I helped build ASTRO, an autonomous robot which will be used in some courses to help students learn ROS2 and automation. My job was to help with hardware assembly and use ROS2 to simulate the robot and its environment
  </p>

  <p>
    I utilise a dual-boot setup with both Ubuntu and Windows 10 on my laptop so I can get the best of both worlds. Software like Altium Designer, Solidworks, Fusion 360 and Cura Slicer run on Windows and others like VS Code, PyCharm, EasyEDA, Arduino IDE and ROS2 on linux.&nbsp;
  </p>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="https://ljmaker.github.io/media/posts/7/ASTROPic1.jpg" height="3072" width="4080" alt=""  sizes="(min-width: 37.5em) 1600px, 80vw" srcset="https://ljmaker.github.io/media/posts/7/responsive/ASTROPic1-xs.jpg 384w ,https://ljmaker.github.io/media/posts/7/responsive/ASTROPic1-sm.jpg 600w ,https://ljmaker.github.io/media/posts/7/responsive/ASTROPic1-md.jpg 768w ,https://ljmaker.github.io/media/posts/7/responsive/ASTROPic1-lg.jpg 1200w ,https://ljmaker.github.io/media/posts/7/responsive/ASTROPic1-xl.jpg 1600w">
      
    </figure>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="https://ljmaker.github.io/media/posts/7/ASTROPic2.jpg" height="3072" width="4080" alt=""  sizes="(min-width: 37.5em) 1600px, 80vw" srcset="https://ljmaker.github.io/media/posts/7/responsive/ASTROPic2-xs.jpg 384w ,https://ljmaker.github.io/media/posts/7/responsive/ASTROPic2-sm.jpg 600w ,https://ljmaker.github.io/media/posts/7/responsive/ASTROPic2-md.jpg 768w ,https://ljmaker.github.io/media/posts/7/responsive/ASTROPic2-lg.jpg 1200w ,https://ljmaker.github.io/media/posts/7/responsive/ASTROPic2-xl.jpg 1600w">
      
    </figure>

  <p>
    Getting familiar with ROS2 interface. Rqt camera image viewer, gazebo interface with a test world and terminator emulator are visible. I started learning ROS2 with turtlebot3 and then moved on to creating interfaces like these.&nbsp;
  </p>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="https://ljmaker.github.io/media/posts/7/Screenshot-from-2025-03-24-20-45-31.png" height="1013" width="1847" alt=""  sizes="(min-width: 37.5em) 1600px, 80vw" srcset="https://ljmaker.github.io/media/posts/7/responsive/Screenshot-from-2025-03-24-20-45-31-xs.png 384w ,https://ljmaker.github.io/media/posts/7/responsive/Screenshot-from-2025-03-24-20-45-31-sm.png 600w ,https://ljmaker.github.io/media/posts/7/responsive/Screenshot-from-2025-03-24-20-45-31-md.png 768w ,https://ljmaker.github.io/media/posts/7/responsive/Screenshot-from-2025-03-24-20-45-31-lg.png 1200w ,https://ljmaker.github.io/media/posts/7/responsive/Screenshot-from-2025-03-24-20-45-31-xl.png 1600w">
      
    </figure>

  <p>
    VS code interface for this particular project&nbsp;
  </p>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="https://ljmaker.github.io/media/posts/7/Screenshot-from-2025-03-24-20-48-45.png" height="1049" width="1831" alt=""  sizes="(min-width: 37.5em) 1600px, 80vw" srcset="https://ljmaker.github.io/media/posts/7/responsive/Screenshot-from-2025-03-24-20-48-45-xs.png 384w ,https://ljmaker.github.io/media/posts/7/responsive/Screenshot-from-2025-03-24-20-48-45-sm.png 600w ,https://ljmaker.github.io/media/posts/7/responsive/Screenshot-from-2025-03-24-20-48-45-md.png 768w ,https://ljmaker.github.io/media/posts/7/responsive/Screenshot-from-2025-03-24-20-48-45-lg.png 1200w ,https://ljmaker.github.io/media/posts/7/responsive/Screenshot-from-2025-03-24-20-48-45-xl.png 1600w">
      
    </figure>

  <p>
    Part of the project code. Launch description generator.
  </p>
<pre class="line-numbers  language-python"><code>import os
from ament_index_python.packages import get_package_share_directory
from launch import LaunchDescription
from launch.actions import IncludeLaunchDescription
from launch.launch_description_sources import PythonLaunchDescriptionSource
from launch_ros.actions import Node

def generate_launch_description():
    package_name = 'articubot_one'

    rsp = IncludeLaunchDescription(
        PythonLaunchDescriptionSource(
            os.path.join(get_package_share_directory(package_name), 'launch', 'rsp.launch.py')
        ),
        launch_arguments={'use_sim_time': 'true', 'use_ros2_control': 'true'}.items()
    )

    joystick = IncludeLaunchDescription(
        PythonLaunchDescriptionSource(
            os.path.join(get_package_share_directory(package_name), 'launch', 'joystick.launch.py')
        ),
        launch_arguments={'use_sim_time': 'true'}.items()
    )

    gazebo_params_file = os.path.join(get_package_share_directory(package_name), 'config', 'gazebo_params.yaml')

    gazebo = IncludeLaunchDescription(
        PythonLaunchDescriptionSource(
            os.path.join(get_package_share_directory('gazebo_ros'), 'launch', 'gazebo.launch.py')
        ),
        launch_arguments={'extra_gazebo_args': f'--ros-args --params-file {gazebo_params_file}'}.items()
    )

    spawn_entity = Node(
        package='gazebo_ros',
        executable='spawn_entity.py',
        arguments=['-topic', 'robot_description', '-entity', 'my_bot'],
        output='screen'
    )

    diff_drive_spawner = Node(
        package='controller_manager',
        executable='spawner.py',
        arguments=['diff_cont']
    )

    joint_broad_spawner = Node(
        package='controller_manager',
        executable='spawner.py',
        arguments=['joint_broad']
    )

    return LaunchDescription([
        rsp,
        gazebo,
        spawn_entity
    ])
</code></pre>
            ]]>
        </content>
    </entry>
    <entry>
        <title>Pytorch</title>
        <author>
            <name>Lovro Joksimović</name>
        </author>
        <link href="https://ljmaker.github.io/pytorch/"/>
        <id>https://ljmaker.github.io/pytorch/</id>
        <media:content url="https://ljmaker.github.io/media/posts/8/Screenshot-from-2025-03-24-19-21-19.png" medium="image" />

        <updated>2025-03-27T18:33:00+01:00</updated>
            <summary>
                <![CDATA[
                        <img src="https://ljmaker.github.io/media/posts/8/Screenshot-from-2025-03-24-19-21-19.png" alt="" />
                    The main goal was to learn the basics of AI by using Pytorch, a Python machine learning library developed by&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                    <p><img src="https://ljmaker.github.io/media/posts/8/Screenshot-from-2025-03-24-19-21-19.png" class="type:primaryImage" alt="" /></p>
                
    <h2 id="the-main-goal-was-to-learn-the-basics-of-ai-by-using-pytorch-a-python-machine-learning-library-developed-by-meta-ai">
      The main goal was to learn the basics of AI by using Pytorch, a Python machine learning library developed by Meta AI
    </h2>

  <p>
    I made several simple projects in Pytorch by watching freeCodeCamp tutorials online and programming in PyCharm. I learned the fundamentals of convolutional neural networks and some of their most important layers like linear, ReLU, MaxPool2d and Conv2d. I also learned what happens during optimisation, forward pass, backward pass (backpropagation), gradient descent and so on. Below are some of my favourite projects of this kind
  </p>

    <h2 id="multiclass-classification-using-2d-points-blobs">
      Multiclass classification using 2d points (blobs)
    </h2>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="https://ljmaker.github.io/media/posts/8/Screenshot-from-2025-03-24-19-27-38.png" height="681" width="1200" alt=""  sizes="(min-width: 37.5em) 1600px, 80vw" srcset="https://ljmaker.github.io/media/posts/8/responsive/Screenshot-from-2025-03-24-19-27-38-xs.png 384w ,https://ljmaker.github.io/media/posts/8/responsive/Screenshot-from-2025-03-24-19-27-38-sm.png 600w ,https://ljmaker.github.io/media/posts/8/responsive/Screenshot-from-2025-03-24-19-27-38-md.png 768w ,https://ljmaker.github.io/media/posts/8/responsive/Screenshot-from-2025-03-24-19-27-38-lg.png 1200w ,https://ljmaker.github.io/media/posts/8/responsive/Screenshot-from-2025-03-24-19-27-38-xl.png 1600w">
      
    </figure>
<pre class="line-numbers  language-python"><code>import torch
from torch import nn
import matplotlib.pyplot as plt
from helper_functions import plot_decision_boundary
from sklearn.datasets import make_blobs
from sklearn.model_selection import train_test_split
from torchmetrics.classification import MulticlassF1Score

device = "cuda" if torch.cuda.is_available() else "cpu"

X_blob, y_blob = make_blobs(n_samples=1000, n_features=2, centers=4, cluster_std=1.5, random_state=42)

X_blob = torch.from_numpy(X_blob).type(torch.float)
y_blob = torch.from_numpy(y_blob).type(torch.long)

X_train, X_test, y_train, y_test = train_test_split(X_blob, y_blob, test_size=0.2, random_state=42)

class BlobModel(nn.Module):
    def __init__(self, input_features, output_features, hidden_units = 8):
        super().__init__()
        self.linear_layer_stack = nn.Sequential(
            nn.Linear(in_features=input_features, out_features=hidden_units),
            nn.ReLU(),
            nn.Linear(in_features=hidden_units, out_features=hidden_units),
            nn.ReLU(),
            nn.Linear(in_features=hidden_units, out_features=output_features)

        )
    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:
        return self.linear_layer_stack(x)

# Number of output features has to be same as the number of classes (4 blob colours)
model5 = BlobModel(input_features=2, output_features=4, hidden_units=8).to(device)
X_train, X_test = X_train.to(device), X_test.to(device)
y_train, y_test = y_train.to(device), y_test.to(device)

loss_func = nn.CrossEntropyLoss()
optimiser = torch.optim.SGD(params=model5.parameters(), lr=0.1)

torch.manual_seed(42)
torch.cuda.manual_seed(42)

epochs = 100

def accuracy(y_true, prediction):
    correct = torch.eq(y_true, prediction).sum().item()
    acc = (correct / len(prediction)) * 100
    return acc

for epoch in range (epochs):
    model5.train()
    logits = model5(X_train)
    y_pred = torch.argmax(torch.softmax(logits, dim=1),dim=1)
    loss = loss_func(logits, y_train)
    f1 = MulticlassF1Score(num_classes=4, average=None).to(device) # Way better than accuracy, combines precision and recall
    f1 = f1(y_pred, y_train)
    optimiser.zero_grad()
    loss.backward()
    optimiser.step()

    model5.eval()
    with torch.inference_mode():
        test_logits = model5(X_test)
        test_pred = torch.softmax(test_logits, dim=1).argmax(dim=1)
        test_loss = loss_func(test_logits, y_test)
        test_f1 = MulticlassF1Score(num_classes=4, average=None).to(device)
        test_f1 = test_f1(test_pred, y_test)

    if epoch % 10 == 0:
        print(f"Epoch: {epoch} | Loss: {loss:.4f} | F1 score: {f1} | Test loss: {test_loss:.4f} | Test F1 score: {test_f1}")



plt.figure(figsize=(12,6))
plt.subplot(1, 2, 1)
plt.title("Training")
plot_decision_boundary(model5, X_train, y_train)
plt.subplot(1, 2, 2)
plt.title("Testing")
plot_decision_boundary(model5, X_test, y_test)
plt.show()</code></pre>

    <h2 id="multiclass-classification-using-images-from-mnist-and-fashionmnist-datasets">
      Multiclass classification using images from MNIST and FashionMNIST datasets
    </h2>

  <p>
    &nbsp; &nbsp; &nbsp; &nbsp; Truths and predictions for images&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Confusion matrices
  </p>

  <div  class="gallery-wrapper">
    <div class="gallery" data-columns="2">
      <figure class="gallery__item">
      <a href="https://ljmaker.github.io/media/posts/8/gallery/Screenshot-from-2025-03-24-19-21-19.png" data-size="802x880">
        <img loading="lazy" src="https://ljmaker.github.io/media/posts/8/gallery/Screenshot-from-2025-03-24-19-21-19-thumbnail.png" height="843" width="768" alt="" >
      </a>
      
    </figure><figure class="gallery__item">
      <a href="https://ljmaker.github.io/media/posts/8/gallery/Screenshot-from-2025-03-24-19-21-41.png" data-size="999x818">
        <img loading="lazy" src="https://ljmaker.github.io/media/posts/8/gallery/Screenshot-from-2025-03-24-19-21-41-thumbnail.png" height="629" width="768" alt="" >
      </a>
      
    </figure><figure class="gallery__item">
      <a href="https://ljmaker.github.io/media/posts/8/gallery/Screenshot-from-2025-03-24-19-18-11.png" data-size="802x880">
        <img loading="lazy" src="https://ljmaker.github.io/media/posts/8/gallery/Screenshot-from-2025-03-24-19-18-11-thumbnail.png" height="843" width="768" alt="" >
      </a>
      
    </figure><figure class="gallery__item">
      <a href="https://ljmaker.github.io/media/posts/8/gallery/Screenshot-from-2025-03-24-19-18-36.png" data-size="1001x782">
        <img loading="lazy" src="https://ljmaker.github.io/media/posts/8/gallery/Screenshot-from-2025-03-24-19-18-36-thumbnail.png" height="600" width="768" alt="" >
      </a>
      
    </figure>
    </div>
  </div>

  <p>
    Code for the MNIST dataset (code for FashionMNIST is very similar)
  </p>
<pre class="line-numbers  language-python"><code>import torch
from torch import nn
from torch.utils.data import DataLoader
from torchvision import datasets
from torchvision.transforms import ToTensor
import matplotlib.pyplot as plt
from tqdm.auto import tqdm
from torchmetrics import ConfusionMatrix
from mlxtend.plotting import plot_confusion_matrix
import random

device = "cuda" if torch.cuda.is_available() else "cpu"

train_data = datasets.MNIST(root="data", train=True, download=True, transform=ToTensor(), target_transform=None)
test_data = datasets.MNIST(root="data", train=False, download=True, transform=ToTensor(), target_transform=None)

train_dataloader = DataLoader(dataset=train_data, batch_size=32, shuffle=True)
test_dataloader = DataLoader(dataset=test_data, batch_size=32, shuffle=False)

class_names = datasets.MNIST.classes

print(class_names)

class MNISTModel(nn.Module):
    def __init__(self, input_shape:int, hidden_units:int, output_shape:int):
        super().__init__()
        self.convblock1 = nn.Sequential(
            nn.Conv2d(in_channels=input_shape, out_channels=hidden_units, kernel_size=(3,3), stride=1, padding=1),
            nn.ReLU(),
            nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units, kernel_size=(3,3), stride=1, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=(2,2))
        )
        self.convblock2 = nn.Sequential(
            nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units, kernel_size=(3,3), stride=1, padding=1),
            nn.ReLU(),
            nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units, kernel_size=(3,3), stride=1, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=(2,2))
        )
        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(in_features=hidden_units*49, out_features=output_shape)
        )
    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:
        x = self.convblock1(x)
        x = self.convblock2(x)
        x = self.classifier(x)
        return x

model3 = MNISTModel(input_shape=1, hidden_units=10, output_shape=len(class_names)).to(device)

loss_func = nn.CrossEntropyLoss()
optimiser = torch.optim.SGD(params=model3.parameters(), lr=0.1)

def accuracy(y_true, y_pred):
    correct = torch.eq(y_true, y_pred).sum().item()
    acc = (correct / len(y_pred)) * 100
    return acc

epochs = 3

for epoch in tqdm(range(epochs)):
    model3.train()
    train_loss, train_acc = 0, 0
    for batch, (train_X, train_y) in enumerate(train_dataloader):
        train_X, train_y = train_X.to(device), train_y.to(device)
        train_pred = model3(train_X)
        loss = loss_func(train_pred, train_y)
        train_loss += loss
        train_acc += accuracy(y_true=train_y, y_pred=train_pred.argmax(dim=1))
        optimiser.zero_grad()
        loss.backward()
        optimiser.step()
    train_loss /= len(train_dataloader)
    train_acc /= len(train_dataloader)
    print(f"Epoch: {epoch} | Training loss: {train_loss:.5f} | Training accuracy: {train_acc:.2f}%")


def visual(test_data):
    plt.figure(figsize=(8,8))
    rows, cols = 4, 4
    for i in range(1, rows*cols+1):
        rand_image, label = test_data[random.randint(0, len(test_data))]
        rand_image = rand_image.unsqueeze(0).to(device)
        rand_pred = model3(rand_image)
        rand_pred = torch.argmax(rand_pred, dim=1)
        plt.subplot(rows, cols, i)
        if rand_pred == label:
            plt.title(f"Truth: {label} | Prediction: {rand_pred.item()}", c="g", fontsize=9)
        else:
            plt.title(f"Truth: {label} | Prediction: {rand_pred.item()}", c="r", fontsize=9)
        plt.imshow(rand_image.squeeze().cpu(), cmap="gray")
        plt.axis(False)
    plt.show()
visual(test_data)

def conf_mat(test_dataloader, test_data, device: torch.device):
    global fig, ax
    model3.eval()
    y_preds = []
    with torch.inference_mode():
        for test_X, test_y in test_dataloader:
            test_X, test_y = test_X.to(device), test_y.to(device)
            test_pred = model3(test_X)
            y_preds.append(test_pred.cpu())
        final_matrix = torch.cat(y_preds)

    confmat = ConfusionMatrix(num_classes=len(class_names), task="multiclass")
    confmat_tensor = confmat(preds=final_matrix, target=test_data.targets)

    fig, ax = plot_confusion_matrix(conf_mat=confmat_tensor.numpy(), class_names=class_names, figsize=(10, 7))
    plt.show()
conf_mat(test_dataloader, test_data, device)
</code></pre>

    <h2 id="i-even-tried-making-my-own-version-of-darknet-19-the-cnn-backbone-of-yolov2">
      I even tried making my own version of Darknet-19, the CNN backbone of YOLOv2
    </h2>

  <p>
    I don't know if it would fail or not (I assume it would since stuff rarely works in the first try in projects like these) since my laptop just couldn't execute the code. It took way too long and started overheating even with cuda enabled. This is also the reason why I only focused on "lighter" datasets like MNIST instead of real image datasets like ImageNet.
  </p>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="https://ljmaker.github.io/media/posts/8/11694_2024_2717_Fig1_HTML.png" height="402" width="1200" alt=""  sizes="(min-width: 37.5em) 1600px, 80vw" srcset="https://ljmaker.github.io/media/posts/8/responsive/11694_2024_2717_Fig1_HTML-xs.png 384w ,https://ljmaker.github.io/media/posts/8/responsive/11694_2024_2717_Fig1_HTML-sm.png 600w ,https://ljmaker.github.io/media/posts/8/responsive/11694_2024_2717_Fig1_HTML-md.png 768w ,https://ljmaker.github.io/media/posts/8/responsive/11694_2024_2717_Fig1_HTML-lg.png 1200w ,https://ljmaker.github.io/media/posts/8/responsive/11694_2024_2717_Fig1_HTML-xl.png 1600w">
      
    </figure>
<pre class="line-numbers  language-python"><code>import torch
from torch import nn
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
import matplotlib.pyplot as plt
from PIL import Image
import random
from torchvision.transforms import Compose, ToTensor, Resize
from torchvision.datasets import ImageFolder
from torch.utils.data import Subset
from sklearn.model_selection import train_test_split
import os
from tqdm.auto import tqdm


device = "cuda" if torch.cuda.is_available() else "cpu"

train_data = datasets.Food101(
    root="data",
    split="train",
    download=True,
    transform=None,
    target_transform=None
)

test_data=datasets.Food101(
    root="data",
    split="test",
    download=True,
    transform=None,
    target_transform=None
)


def train_val_dataset(dataset, val_split=0.25):
    train_idx, val_idx = train_test_split(list(range(len(dataset))), test_size=val_split)
    datasets = {}
    datasets['train'] = Subset(dataset, train_idx)
    datasets['val'] = Subset(dataset, val_idx)
    return datasets

data_transform = transforms.Compose([
    transforms.Resize(size=(448, 448)),
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.ToTensor()
])

dataset = ImageFolder("/home/nemo/PycharmProjects/PytorchComputerVision/data/food-101/images", transform=data_transform)
datasets = train_val_dataset(dataset)
print(len(datasets['train']))
print(len(datasets['val']))

train_dataloader = DataLoader(dataset=datasets['train'], shuffle=True, num_workers=os.cpu_count(), batch_size=32, drop_last=True)
test_dataloader = DataLoader(dataset=datasets['val'], shuffle=False, num_workers=os.cpu_count(), batch_size=32, drop_last=True)

image, label = datasets["train"][0]
image = image.to(device)
print(image.shape)

class_names = dataset.classes

def visual(image, label):
    plt.figure(figsize=(12, 6))
    plt.title(class_names[label])
    plt.imshow(image.permute(1, 2, 0))
    plt.axis("off")
    plt.show()
#visual(image, label)

class Darknet19(nn.Module):
    def __init__(self):
        super().__init__()
        self.convblock1 = nn.Sequential(
            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.BatchNorm2d(32)
        )
        self.convblock2 = nn.Sequential(
            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.BatchNorm2d(64)
        )
        self.convblock3 = nn.Sequential(
            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.Conv2d(in_channels=128, out_channels=64, kernel_size=1, stride=1, padding=0),
            nn.ReLU(),
            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.BatchNorm2d(128)
        )
        self.convblock4 = nn.Sequential(
            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.Conv2d(in_channels=256, out_channels=128, kernel_size=1, stride=1, padding=0),
            nn.ReLU(),
            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.BatchNorm2d(256)
        )
        self.convblock5 = nn.Sequential(
            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.Conv2d(in_channels=512, out_channels=256, kernel_size=1, stride=1, padding=0),
            nn.ReLU(),
            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.Conv2d(in_channels=512, out_channels=256, kernel_size=1, stride=1, padding=0),
            nn.ReLU(),
            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.BatchNorm2d(512)
        )
        self.convblock6 = nn.Sequential(
            nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.Conv2d(in_channels=1024, out_channels=512, kernel_size=1, stride=1, padding=0),
            nn.ReLU(),
            nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.Conv2d(in_channels=1024, out_channels=512, kernel_size=1, stride=1, padding=0),
            nn.ReLU(),
            nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(1024)

        )
        self.convblock7 = nn.Sequential(
            nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.AvgPool2d(kernel_size=2),
            nn.BatchNorm2d(1024)
        )

        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(in_features=1024*49, out_features=6),
            nn.Softmax(dim=1),

        )
    def forward(self, x:torch.Tensor) -&gt; torch.Tensor:
        x = self.convblock1(x)
        x = self.convblock2(x)
        x = self.convblock3(x)
        x = self.convblock4(x)
        x = self.convblock5(x)
        x = self.convblock6(x)
        x = self.convblock7(x)
        x = self.classifier(x)
        return x



model5 = Darknet19().to(device)


loss_func = nn.CrossEntropyLoss()
optimiser = torch.optim.Adam(params=model5.parameters(), lr=0.0001)

def accuracy(y_true, y_pred):
    correct = torch.eq(y_true, y_pred).sum().item()
    acc = (correct / len(y_pred)) * 100
    return acc

def train_step(model: torch.nn.Module, dataloader: torch.utils.data.DataLoader, loss_function: torch.nn.Module,
               optimiser: torch.optim.Optimizer, accuracy, device: torch.device):
    train_loss, train_acc = 0, 0
    model.train()
    for batch, (train_X, train_y) in enumerate(dataloader):
        train_X, train_y = train_X.to(device), train_y.to(device)
        y_pred = model(train_X)
        loss = loss_function(y_pred, train_y)
        train_loss += loss.item()
        train_acc = accuracy(y_true=train_y, y_pred=y_pred.argmax(dim=1))
        optimiser.zero_grad()
        loss.backward()
        optimiser.step()
    train_loss / len(dataloader)
    train_acc / len(dataloader)
    print(f"Train loss: {train_loss} | Train acc: {train_acc}")


def test_step(model: torch.nn.Module, dataloader: torch.utils.data.DataLoader, loss_function: torch.nn.Module,
              accuracy, device: torch.device):
    final_loss, test_acc = 0, 0
    model.eval()
    with torch.inference_mode():
        for test_X, test_y in dataloader:
            test_X, test_y = test_X.to(device), test_y.to(device)
            test_pred = model(test_X)
            test_loss = loss_function(test_pred, test_y)
            final_loss += test_loss.item()
            test_acc = accuracy(y_true=test_y, y_pred=test_pred.argmax(dim=1))
    test_loss /= len(dataloader)
    test_acc /= len(dataloader)
    print(f"Test loss: {test_loss} | Test acc: {test_acc}")

epochs = 5



for epoch in tqdm(range(epochs)):
    print(f"Epoch: {epoch}")
    print("---------------")
    train_step(model=model5, dataloader=train_dataloader, loss_function=loss_func, optimiser=optimiser, accuracy=accuracy, device=device)
    test_step(model=model5, dataloader=test_dataloader, loss_function=loss_func, accuracy=accuracy, device=device)</code></pre>

    <h2 id="landmark-detection-using-mediapipe-hands-dataset">
      Landmark detection using MediaPipe Hands dataset
    </h2>

  <p>
    I wanted to experiment with this dataset and make it detect the middle points between all the fingers on my hand
  </p>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="https://ljmaker.github.io/media/posts/8/Screenshot-from-2025-03-24-19-12-25.png" height="1046" width="1845" alt=""  sizes="(min-width: 37.5em) 1600px, 80vw" srcset="https://ljmaker.github.io/media/posts/8/responsive/Screenshot-from-2025-03-24-19-12-25-xs.png 384w ,https://ljmaker.github.io/media/posts/8/responsive/Screenshot-from-2025-03-24-19-12-25-sm.png 600w ,https://ljmaker.github.io/media/posts/8/responsive/Screenshot-from-2025-03-24-19-12-25-md.png 768w ,https://ljmaker.github.io/media/posts/8/responsive/Screenshot-from-2025-03-24-19-12-25-lg.png 1200w ,https://ljmaker.github.io/media/posts/8/responsive/Screenshot-from-2025-03-24-19-12-25-xl.png 1600w">
      
    </figure>
<pre class="line-numbers  language-python"><code>import cv2  # for reading image paths and drawing on images
import mediapipe as mp  # for importing the hands tracking solution
import matplotlib.pyplot as plt  # to show images

mp_hands = mp.solutions.hands
hands = mp_hands.Hands(static_image_mode=True, max_num_hands=2, min_tracking_confidence=0.7,
                       min_detection_confidence=0.7)  # finetune hyperparameters
mp_drawing = mp.solutions.drawing_utils  # utilise landmark drawing option

# read image path:
sample_img = cv2.imread("/home/nemo/PycharmProjects/ObjectDetection/data/images/2023-09-24-141225.jpg")

chosenHandPoints = [3, 7, 11, 15, 19]  # list of optimal hand points that surround the knife hit points
chosenCoordinates = []  # list for coordinates of the chosen hand points
targetCoordinates = []  # list for the coordinates of knife hit points
coordinate2 = 1

imgRGB = cv2.cvtColor(sample_img, cv2.COLOR_BGR2RGB)  # openCV uses BGR order while matplotlib uses RGB
results = hands.process(imgRGB)
if results.multi_hand_landmarks:
    for handLms in results.multi_hand_landmarks:  # to track both hands
        for points_id, lm in enumerate(handLms.landmark):
            h, w, c = sample_img.shape  # store shape of image
            x, y = int(lm.x * w), int(lm.y * h)  # turns the image ratio into actual pixel coordinates

            if points_id in chosenHandPoints:
                chosenCoordinates.append([x, y])  # store coordinates of chosen points in the list

        # used to draw connections from one point to another:
        mp_drawing.draw_landmarks(sample_img, handLms, mp_hands.HAND_CONNECTIONS)


for coordinate in range(len(chosenCoordinates)-1):
    # calculates x and y coordinates of knife hit points:
    cx = int((chosenCoordinates[coordinate][0] + chosenCoordinates[coordinate2][0]) / 2)
    cy = int((chosenCoordinates[coordinate][1] + chosenCoordinates[coordinate2][1]) / 2)
    targetCoordinates.append([cx, cy])  # stores coordinates of knife hit points into list
    coordinate2 += 1

# Calculating the locations of the hit points that are not inbetween fingers and storing them in the list:
leftDotX = int(2 * chosenCoordinates[0][0] - (chosenCoordinates[0][0] + chosenCoordinates[1][0]) / 2)
leftDotY = int(2 * chosenCoordinates[0][1] - (chosenCoordinates[0][1] + chosenCoordinates[1][1]) / 2)
targetCoordinates.insert(0, [leftDotX, leftDotY])

rightDotX = int(2 * chosenCoordinates[4][0] - (chosenCoordinates[3][0] + chosenCoordinates[4][0]) / 2)
rightDotY = int(2 * chosenCoordinates[4][1] - (chosenCoordinates[3][1] + chosenCoordinates[4][1]) / 2)
targetCoordinates.append([rightDotX, rightDotY])

# goes through the knife hit coordinates list and marks the locations on the image:
for i in range(len(targetCoordinates)):
    cv2.circle(sample_img, (targetCoordinates[i][0], targetCoordinates[i][1]), 15, (255, 0, 0), cv2.FILLED)

print(targetCoordinates)

# plotting the image using matplotlib:

plt.figure(figsize=(10, 10))
plt.axis("off")
plt.title("Resulting image")
plt.imshow(sample_img[:, :, ::-1])
plt.show()</code></pre>
            ]]>
        </content>
    </entry>
    <entry>
        <title>MAMBA</title>
        <author>
            <name>Lovro Joksimović</name>
        </author>
        <link href="https://ljmaker.github.io/mamba/"/>
        <id>https://ljmaker.github.io/mamba/</id>
        <media:content url="https://ljmaker.github.io/media/posts/5/MAMBAShowcase.jpg" medium="image" />

        <updated>2025-03-23T19:04:04+01:00</updated>
            <summary>
                <![CDATA[
                        <img src="https://ljmaker.github.io/media/posts/5/MAMBAShowcase.jpg" alt="" />
                    The main goal was to learn various things, from inverse kinematics to AI implementation and robot arm mechanics All 3D&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                    <p><img src="https://ljmaker.github.io/media/posts/5/MAMBAShowcase.jpg" class="type:primaryImage" alt="" /></p>
                
    <h2 id="the-main-goal-was-to-learn-various-things-from-inverse-kinematics-to-ai-implementation-and-robot-arm-mechanics">
      The main goal was to learn various things, from inverse kinematics to AI implementation and robot arm mechanics
    </h2>

  <p>
    All 3D printed parts were made from PETG except gears which were made from PCTG which has better mechanical properties and is in turn more expensive. Mechanically MAMBA is finished, though I will be making a few adjustments to the current design. I also finished designing all the PCB's, but at the time of writing this I am still waiting for some components to arrive. Every component was designed by myself, excluding the reduction mechanism for motors 2 and 3 which I downloaded from Cults3D and redesigned to fit my assembly.
  </p>

    <h2 id="main-partsandnbsp">
      Main parts&nbsp;
    </h2>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="https://ljmaker.github.io/media/posts/5/MAMBAClearAnnot.png" height="825" width="1273" alt=""  sizes="(min-width: 37.5em) 1600px, 80vw" srcset="https://ljmaker.github.io/media/posts/5/responsive/MAMBAClearAnnot-xs.png 384w ,https://ljmaker.github.io/media/posts/5/responsive/MAMBAClearAnnot-sm.png 600w ,https://ljmaker.github.io/media/posts/5/responsive/MAMBAClearAnnot-md.png 768w ,https://ljmaker.github.io/media/posts/5/responsive/MAMBAClearAnnot-lg.png 1200w ,https://ljmaker.github.io/media/posts/5/responsive/MAMBAClearAnnot-xl.png 1600w">
      
    </figure>
<div><iframe loading="lazy" 
  src="https://drive.google.com/file/d/1Sn3flE7q0tu3qIdmgv2aToYYqTYcRX3i/preview" 
  width="640" 
  height="360" 
  frameborder="0" 
  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" 
  allowfullscreen>
</iframe></div>

  <p>
    The arm is almost identical to the Fortec model, with the only adjustments being the reduction mechanisms for motors 1, 2 and 3 and some minor appearance changes.
  </p>

  <p>
    For motor 1 (rotation of the base of the arm) I used&nbsp;eccentrically cycloidal gears for simplicity. MAMBA used the same design as example a in this picture
  </p>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="https://ljmaker.github.io/media/posts/5/10010_2023_674_Fig1_HTML.png" height="262" width="685" alt=""  sizes="(min-width: 37.5em) 1600px, 80vw" srcset="https://ljmaker.github.io/media/posts/5/responsive/10010_2023_674_Fig1_HTML-xs.png 384w ,https://ljmaker.github.io/media/posts/5/responsive/10010_2023_674_Fig1_HTML-sm.png 600w ,https://ljmaker.github.io/media/posts/5/responsive/10010_2023_674_Fig1_HTML-md.png 768w ,https://ljmaker.github.io/media/posts/5/responsive/10010_2023_674_Fig1_HTML-lg.png 1200w ,https://ljmaker.github.io/media/posts/5/responsive/10010_2023_674_Fig1_HTML-xl.png 1600w">
      
    </figure>

  <p>
    For motors 2 and 3 I only changed the gear ratio and case of a planetary reduction mechanism I found online because I did not have anymore time left to design my own. Prior to this I spent almost a month trying to design a 3D printed harmonic drive which kept failing because the flexspline was too fragile when printed with the filaments I can currently print
  </p>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="https://ljmaker.github.io/media/posts/5/fb-seies-structure.jpg" height="1088" width="2167" alt=""  sizes="(min-width: 37.5em) 1600px, 80vw" srcset="https://ljmaker.github.io/media/posts/5/responsive/fb-seies-structure-xs.jpg 384w ,https://ljmaker.github.io/media/posts/5/responsive/fb-seies-structure-sm.jpg 600w ,https://ljmaker.github.io/media/posts/5/responsive/fb-seies-structure-md.jpg 768w ,https://ljmaker.github.io/media/posts/5/responsive/fb-seies-structure-lg.jpg 1200w ,https://ljmaker.github.io/media/posts/5/responsive/fb-seies-structure-xl.jpg 1600w">
      
    </figure>

  <p>
    
  </p>
<div><iframe loading="lazy" 
  src="https://drive.google.com/file/d/11HXBIqOPpE0uNU0JRdpEstB-0TcArew9/preview" 
  width="640" 
  height="360" 
  frameborder="0" 
  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" 
  allowfullscreen>
</iframe>

</div>

    <h2 id="adding-a-counterbalance-spring-was-the-easiest-way-for-me-to-deal-with-balancing-problems">
      Adding a counterbalance spring was the easiest way for me to deal with balancing problems
    </h2>

  <p>
    The arm doesn't have trouble extending fully because I chose a spring which can withstand up to 15 kg of mass. The spring is compressed with a rod that goes through the coil center and connects to the upper arm via a 6mm rod. That way when the arm extends the spring produces a counterbalancing effect and keeps the arm in place.&nbsp;
  </p>

    <h2 id="all-the-ball-bearings-used-in-this-project-were-bought-from-roto-sisak-except-the-biggest-one-in-the-base-which-i-3d-printed">
      All the ball bearings used in this project were bought from Roto-Sisak, except the biggest one in the base which I 3d printed
    </h2>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="https://ljmaker.github.io/media/posts/5/1742771727171.jpg" height="6144" width="8160" alt=""  sizes="(min-width: 37.5em) 1600px, 80vw" srcset="https://ljmaker.github.io/media/posts/5/responsive/1742771727171-xs.jpg 384w ,https://ljmaker.github.io/media/posts/5/responsive/1742771727171-sm.jpg 600w ,https://ljmaker.github.io/media/posts/5/responsive/1742771727171-md.jpg 768w ,https://ljmaker.github.io/media/posts/5/responsive/1742771727171-lg.jpg 1200w ,https://ljmaker.github.io/media/posts/5/responsive/1742771727171-xl.jpg 1600w">
      <figcaption>My custom 3d printed ball bearing is visible through the opening in the rotator base, which I later made to easily apply lubricating oil if needed. The spring case is visible right next to it</figcaption>
    </figure>

  <p>
    I made the decision to make this ball bearing mainly to cut costs in the projects but it introduces some problems in the current design. It causes a slight tilt in the rotator of the arm, which I plan to fix by replacing the ball bearing with a real metal one.&nbsp;
  </p>

    <h2 id="lower-arm-test-sequence">
      Lower arm test sequence
    </h2>
<div><iframe loading="lazy" 
  src="https://drive.google.com/file/d/1gWtjw2GmgqOrUfWgE-kFme3FVkPRagCB/preview" 
  width="640" 
  height="360" 
  frameborder="0" 
  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" 
  allowfullscreen>
</iframe></div>

    <h2 id="motors-4-5-and-6-control-all-the-necessary-parts-of-the-wrist-motor-4-controls-the-wrist-pitch-motor-5-wrist-yaw-and-motor-6-wrist-roll">
      Motors 4, 5 and 6 control all the necessary parts of the wrist. Motor 4 controls the wrist pitch, motor 5 wrist yaw and motor 6 wrist roll
    </h2>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="https://ljmaker.github.io/media/posts/5/Spherical-Wrist-Model.ppm" height="308" width="652" alt="" >
      
    </figure>

  <p>
    I used lubricating grease on all the gears to further help with movement, which worked especially well for the inner gear which rotates and causes wrist yaw. The universal joints were needed because the upper part is more narrow towards the wrist. The rods used throughout this arm are standard 6mm aluminium rods which I had to grind in places where they were going through the ball bearings. This is because I couldn't find ones with a 5 mm diameter and my local supplier only had a steady supply of ball bearings with that exact inner diameter so I had to adjust to that.&nbsp;
  </p>

  <div  class="gallery-wrapper">
    <div class="gallery" data-columns="2">
      <figure class="gallery__item">
      <a href="https://ljmaker.github.io/media/posts/5/gallery/1742773184905-2.jpg" data-size="8160x6144">
        <img loading="lazy" src="https://ljmaker.github.io/media/posts/5/gallery/1742773184905-2-thumbnail.jpg" height="578" width="768" alt="" >
      </a>
      
    </figure><figure class="gallery__item">
      <a href="https://ljmaker.github.io/media/posts/5/gallery/1742773184846-2.jpg" data-size="8160x6144">
        <img loading="lazy" src="https://ljmaker.github.io/media/posts/5/gallery/1742773184846-2-thumbnail.jpg" height="578" width="768" alt="" >
      </a>
      
    </figure>
    </div>
  </div>

  <p id="the-first-set-of-timing-pulleys-is-directly-connected-with-the-rods-while-the-other-set-connects-with-the-first-via-two-timing-belts-that-slide-on-idlers">
    The first set of timing pulleys is directly connected with the rods, while the other set connects with the first via two timing belts that slide on idlers
  </p>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="https://ljmaker.github.io/media/posts/5/1742773842367.jpg" height="6144" width="8160" alt=""  sizes="(min-width: 37.5em) 1600px, 80vw" srcset="https://ljmaker.github.io/media/posts/5/responsive/1742773842367-xs.jpg 384w ,https://ljmaker.github.io/media/posts/5/responsive/1742773842367-sm.jpg 600w ,https://ljmaker.github.io/media/posts/5/responsive/1742773842367-md.jpg 768w ,https://ljmaker.github.io/media/posts/5/responsive/1742773842367-lg.jpg 1200w ,https://ljmaker.github.io/media/posts/5/responsive/1742773842367-xl.jpg 1600w">
      
    </figure>

  <p id="one-of-the-most-complicated-parts-mechanically-was-designing-two-bevel-gears-which-transfer-rotation-to-the-spur-gears-which-rotate-the-second-inner-gear">
    One of the most complicated parts mechanically was designing two bevel gears which transfer rotation to the spur gears that rotate the second inner gear.&nbsp;First sets kept breaking and bending so I ended up supergluing them and greasing them heavily. This whole wrist (or gearbox as I named it in Solidworks) was very challenging for me to design and assemble because I was working with very tight spaces.
  </p>

    <figure class="post__image post__image--center">
      <img loading="lazy" src="https://ljmaker.github.io/media/posts/5/1742771727233.jpg" height="6144" width="8160" alt=""  sizes="(min-width: 37.5em) 1600px, 80vw" srcset="https://ljmaker.github.io/media/posts/5/responsive/1742771727233-xs.jpg 384w ,https://ljmaker.github.io/media/posts/5/responsive/1742771727233-sm.jpg 600w ,https://ljmaker.github.io/media/posts/5/responsive/1742771727233-md.jpg 768w ,https://ljmaker.github.io/media/posts/5/responsive/1742771727233-lg.jpg 1200w ,https://ljmaker.github.io/media/posts/5/responsive/1742771727233-xl.jpg 1600w">
      
    </figure>

    <h2 id="upper-arm-test-sequence">
      Upper arm test sequence
    </h2>
<div><iframe loading="lazy" 
  src="https://drive.google.com/file/d/1VYWuKPTHg9pUNqK0NRDb1b1_6bcbaIy-/preview" 
  width="640" 
  height="360" 
  frameborder="0" 
  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" 
  allowfullscreen>
</iframe></div>

  <p>
    It took me a total of 9 months to design and test all the mechanics, though I did not work on this full time and took some breaks as well.&nbsp;
  </p>

  <div  class="gallery-wrapper">
    <div class="gallery" data-columns="3">
      <figure class="gallery__item">
      <a href="https://ljmaker.github.io/media/posts/5/gallery/MAMBAPieces-2.jpg" data-size="8160x6144">
        <img loading="lazy" src="https://ljmaker.github.io/media/posts/5/gallery/MAMBAPieces-2-thumbnail.jpg" height="578" width="768" alt="" >
      </a>
      
    </figure><figure class="gallery__item">
      <a href="https://ljmaker.github.io/media/posts/5/gallery/IMG_20241006_205902.jpg" data-size="4080x3072">
        <img loading="lazy" src="https://ljmaker.github.io/media/posts/5/gallery/IMG_20241006_205902-thumbnail.jpg" height="578" width="768" alt="" >
      </a>
      
    </figure>
    </div>
  </div>

  <p>
    Some color combinations I was testing out in software during the different stages of design
  </p>

  <div  class="gallery-wrapper">
    <div class="gallery" data-columns="2">
      <figure class="gallery__item">
      <a href="https://ljmaker.github.io/media/posts/5/gallery/Screenshot-2024-11-28-005447.png" data-size="1910x916">
        <img loading="lazy" src="https://ljmaker.github.io/media/posts/5/gallery/Screenshot-2024-11-28-005447-thumbnail.png" height="368" width="768" alt="" >
      </a>
      
    </figure><figure class="gallery__item">
      <a href="https://ljmaker.github.io/media/posts/5/gallery/Screenshot-2024-12-11-145317.png" data-size="1177x552">
        <img loading="lazy" src="https://ljmaker.github.io/media/posts/5/gallery/Screenshot-2024-12-11-145317-thumbnail.png" height="360" width="768" alt="" >
      </a>
      
    </figure><figure class="gallery__item">
      <a href="https://ljmaker.github.io/media/posts/5/gallery/Screenshot-2024-11-28-003349.png" data-size="1914x918">
        <img loading="lazy" src="https://ljmaker.github.io/media/posts/5/gallery/Screenshot-2024-11-28-003349-thumbnail.png" height="368" width="768" alt="" >
      </a>
      
    </figure>
    </div>
  </div>

    <h2 id="the-electronics-block-consists-of-a-raspberry-pi-5-a-logic-board-which-will-have-an-esp32-and-a-driver-board-which-has-individual-custom-made-driver-board-pcbsandnbsp">
      The electronics block consists of a raspberry pi 5, a logic board which will have an ESP32 and a driver board which has individual custom made driver board pcb's.&nbsp;
    </h2>

  <div  class="gallery-wrapper">
    <div class="gallery" data-columns="2">
      <figure class="gallery__item">
      <a href="https://ljmaker.github.io/media/posts/5/gallery/MAMBABoardStackAnnot.png" data-size="1004x728">
        <img loading="lazy" src="https://ljmaker.github.io/media/posts/5/gallery/MAMBABoardStackAnnot-thumbnail.png" height="557" width="768" alt="" >
      </a>
      
    </figure><figure class="gallery__item">
      <a href="https://ljmaker.github.io/media/posts/5/gallery/MAMBAHardwareBlock.jpg" data-size="8160x6144">
        <img loading="lazy" src="https://ljmaker.github.io/media/posts/5/gallery/MAMBAHardwareBlock-thumbnail.jpg" height="578" width="768" alt="" >
      </a>
      
    </figure>
    </div>
  </div>

  <p>
    I decided to try to make my own modular driver board simply because I wanted to learn how they operate, so I reverse engineered it from a schematic of a similar one I found online. I settled on TMC2225 stepper motor driver because they were really cheap on Mouser Electronics considering they are Trinamic drivers
  </p>

  <div  class="gallery-wrapper">
    <div class="gallery" data-columns="3">
      <figure class="gallery__item">
      <a href="https://ljmaker.github.io/media/posts/5/gallery/MAMBADriverTop.png" data-size="781x656">
        <img loading="lazy" src="https://ljmaker.github.io/media/posts/5/gallery/MAMBADriverTop-thumbnail.png" height="645" width="768" alt="" >
      </a>
      
    </figure><figure class="gallery__item">
      <a href="https://ljmaker.github.io/media/posts/5/gallery/MAMBADriverBottom.png" data-size="694x664">
        <img loading="lazy" src="https://ljmaker.github.io/media/posts/5/gallery/MAMBADriverBottom-thumbnail.png" height="664" width="694" alt="" >
      </a>
      
    </figure><figure class="gallery__item">
      <a href="https://ljmaker.github.io/media/posts/5/gallery/MAMBAProof.jpg" data-size="8160x6144">
        <img loading="lazy" src="https://ljmaker.github.io/media/posts/5/gallery/MAMBAProof-thumbnail.jpg" height="578" width="768" alt="" >
      </a>
      
    </figure>
    </div>
  </div>

  <p>
    Raspberry pi will communicate with the ESP32 via a USB port and the microcontroller will control all the drivers on the driver board. I also mounted an additional cooling fan to provide active cooling to the motors. One of the components I currently don't have is the proper ESP32 because I originally ordered the one which has PSRAM inlcuded, which blocks 3 pins from use which I absolutely need since I plan to use almost all of them. Each motor has 3 pins (STEP, DIR, EN) and each encoder 2 pins. In total in this project I will be using 32 pins of the ESP32 S3-WROOM1-N8 microcontroller.
  </p>

  <div  class="gallery-wrapper">
    <div class="gallery" data-columns="2">
      <figure class="gallery__item">
      <a href="https://ljmaker.github.io/media/posts/5/gallery/MAMBALogic.png" data-size="1278x806">
        <img loading="lazy" src="https://ljmaker.github.io/media/posts/5/gallery/MAMBALogic-thumbnail.png" height="484" width="768" alt="" >
      </a>
      
    </figure><figure class="gallery__item">
      <a href="https://ljmaker.github.io/media/posts/5/gallery/MAMBAMainDriver.png" data-size="1043x729">
        <img loading="lazy" src="https://ljmaker.github.io/media/posts/5/gallery/MAMBAMainDriver-thumbnail.png" height="537" width="768" alt="" >
      </a>
      
    </figure>
    </div>
  </div>

  <p>
    I made a hierarchical schematic for the driver board because it was the easiest way for me to add all 6 of the individual driver modules while keeping it well organised. It is a really nice feature which saved me a lot of time.
  </p>

  <div  class="gallery-wrapper">
    <div class="gallery" data-columns="3">
      <figure class="gallery__item">
      <a href="https://ljmaker.github.io/media/posts/5/gallery/Screenshot-2025-03-20-225157.png" data-size="990x668">
        <img loading="lazy" src="https://ljmaker.github.io/media/posts/5/gallery/Screenshot-2025-03-20-225157-thumbnail.png" height="518" width="768" alt="" >
      </a>
      
    </figure><figure class="gallery__item">
      <a href="https://ljmaker.github.io/media/posts/5/gallery/Screenshot-2025-03-20-225147.png" data-size="814x517">
        <img loading="lazy" src="https://ljmaker.github.io/media/posts/5/gallery/Screenshot-2025-03-20-225147-thumbnail.png" height="488" width="768" alt="" >
      </a>
      
    </figure>
    </div>
  </div>

  <p id="since-i-dont-have-all-the-necessary-components-i-made-a-simple-driver-board-can-only-control-3-motor-at-a-time-because-arduino-uno-doesnt-have-enough-pins-for-all-6-the-one-on-the-right-was-a-test-run-for-the-bigger-board">
    Since I didn’t have all the necessary components, I built a simple driver board that can control only three motors at a time due to the Arduino UNO’s limited number of pins—not enough for all six motors. The board on the right was a test run for a larger version. For power, I repurposed an old ATX power supply from a spare computer. It delivers 12V at 10A, which should be sufficient to run all 6 motors, as each motor will draw around 1.3 to 1.5A.
  </p>

  <div  class="gallery-wrapper">
    <div class="gallery" data-columns="2">
      <figure class="gallery__item">
      <a href="https://ljmaker.github.io/media/posts/5/gallery/MAMBA3Drivers-2.jpg" data-size="8160x6144">
        <img loading="lazy" src="https://ljmaker.github.io/media/posts/5/gallery/MAMBA3Drivers-2-thumbnail.jpg" height="578" width="768" alt="" >
      </a>
      
    </figure>
    </div>
  </div>

  <p id="in-the-future-i-plan-to-integrate-ros2-and-ai-into-the-project-as-well-as-make-a-proper-software-behind-this-currently-it-is-still-in-the-testing-phase">
    In the future I plan to integrate ROS2 and AI into the project as well as make a proper software behind this. Currently it is still in the testing phase. I even bought an OAK-D-LITE camera which I plan to put onto the tool plate.
  </p>

  <p>
    This is the code for the lower arm sequence. The code for the upper arm sequence is very similar.
  </p>
<pre class="line-numbers  language-cpp"><code>const int stepPins[] = {4, 7, 11};
const int dirPins[] = {3, 6, 9};
const int enaPins[] = {5, 8, 12};

const long intervals[] = {2000, 2500, 1500}; 
const int speeds[] = {600, 400, 500};  

bool direction = HIGH;

void moveMotor(int motorIndex, long interval, int stepDelay) {
    digitalWrite(enaPins[motorIndex], LOW);  
    digitalWrite(dirPins[motorIndex], direction);
    Serial.print("Motor ");
    Serial.print(motorIndex + 1);
    Serial.println(" Rotating Forward");
   
    unsigned long startTime = millis();
    while (millis() - startTime &lt; interval) {
        digitalWrite(stepPins[motorIndex], HIGH);
        delayMicroseconds(stepDelay);
        digitalWrite(stepPins[motorIndex], LOW);
        delayMicroseconds(stepDelay);
    }

   
    direction = !direction;
    digitalWrite(dirPins[motorIndex], direction);
    Serial.print("Motor ");
    Serial.print(motorIndex + 1);
    Serial.println(" Rotating Reverse");
   
    startTime = millis();
    while (millis() - startTime &lt; interval) {
        digitalWrite(stepPins[motorIndex], HIGH);
        delayMicroseconds(stepDelay);
        digitalWrite(stepPins[motorIndex], LOW);
        delayMicroseconds(stepDelay);
    }
}

void setup() {
    Serial.begin(9600);
   
    for (int i = 0; i &lt; 3; i++) {
        pinMode(stepPins[i], OUTPUT);
        pinMode(dirPins[i], OUTPUT);
        pinMode(enaPins[i], OUTPUT);
        moveMotor(i, intervals[i], speeds[i]);
    }
}

void loop() {
}</code></pre>

  <p>
    For this project I primarily used Altium Designer and Solidworks, though I did start designing it in Fusion 360 at first but switched to Solidworks after I got the free license for it license from my faculty.
  </p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>About me</title>
        <author>
            <name>Lovro Joksimović</name>
        </author>
        <link href="https://ljmaker.github.io/about-me/"/>
        <id>https://ljmaker.github.io/about-me/</id>
        <media:content url="https://ljmaker.github.io/media/posts/1/PicZaAboutMe2-2.jpg" medium="image" />

        <updated>2025-03-23T16:42:53+01:00</updated>
            <summary>
                <![CDATA[
                        <img src="https://ljmaker.github.io/media/posts/1/PicZaAboutMe2-2.jpg" alt="" />
                    Hi! My name is Lovro Joksimović. I'm a second-year Mechatronics and Robotics student from Croatia, studying at the Faculty of&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                    <p><img src="https://ljmaker.github.io/media/posts/1/PicZaAboutMe2-2.jpg" class="type:primaryImage" alt="" /></p>
                <p class="" data-start="153" data-end="503">Hi! My name is Lovro Joksimović. I'm a second-year Mechatronics and Robotics student from Croatia, studying at the Faculty of Mechanical Engineering and Naval Architecture in Zagreb. I am passionate about building engineering projects in my spare time, particularly in the field of robotics and automation, and I have built more than 10 projects since I was 14.</p>
<figure class="post__image"><img loading="lazy"  src="https://ljmaker.github.io/media/posts/1/PicZaAboutMe.jpg" alt="" width="3264" height="2448" sizes="(min-width: 37.5em) 1600px, 80vw" srcset="https://ljmaker.github.io/media/posts/1/responsive/PicZaAboutMe-xs.jpg 384w ,https://ljmaker.github.io/media/posts/1/responsive/PicZaAboutMe-sm.jpg 600w ,https://ljmaker.github.io/media/posts/1/responsive/PicZaAboutMe-md.jpg 768w ,https://ljmaker.github.io/media/posts/1/responsive/PicZaAboutMe-lg.jpg 1200w ,https://ljmaker.github.io/media/posts/1/responsive/PicZaAboutMe-xl.jpg 1600w"></figure>
<p class="" data-start="505" data-end="757">Beyond engineering, I enjoy playing football as a backup goalkeeper for my local club, hitting the gym, and watching movies. I am always eager to learn, experiment and bring ideas to life through hands-on projects and experience.</p>
            ]]>
        </content>
    </entry>
</feed>
